{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 네이버API 사용 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl \"https://openapi.naver.com/v1/papago/n2mt\" \\\n",
    "-H \"Content-Type: application/x-www-form-urlencoded; charset=UTF-8\" \\\n",
    "-H \"X-Naver-Client-Id: vsdXXZ9ntwAeFeejf4qC\" \\\n",
    "-H \"X-Naver-Client-Secret: zHa2LO_TlI\" \\\n",
    "-d \"source=ko&target=en&text=만나서 반갑습니다.\" -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"YOUR_CLIENT_ID\"\n",
    "client_secret = \"YOUR_CLIENT_SECRET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색 API예제는 블로그를 비롯 전문자료까지 호출방법이 동일하므로 blog검색만 대표로 예제를 올렸습니다.\n",
    "# 네이버 검색 Open API 예제 - 블로그 검색\n",
    "\n",
    "client_id = \"YOUR_CLIENT_ID\"\n",
    "client_secret = \"YOUR_CLIENT_SECRET\"\n",
    "encText = urllib.parse.quote(\"검색할 단어\")\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text = eval(response_body)\n",
    "json_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text['display']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text['items'][0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text['items'][0]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_text['items'][0]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas 데이터프레임 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['items'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_df['items'][0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['items'][0]['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버API - 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네이버 검색 Open API 예제 - 뉴스 검색\n",
    "\n",
    "client_id = \"YOUR_CLIENT_ID\"\n",
    "client_secret = \"YOUR_CLIENT_SECRET\"\n",
    "encText = urllib.parse.quote(\"트와이스\")\n",
    "url = \"https://openapi.naver.com/v1/search/news?query=\" + encText # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_enter = 'https://entertain.naver.com/read?oid=417&aid=0000458809'\n",
    "print(url_enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "html = requests.get(url_enter)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(html.text, 'lxml')\n",
    "print(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title 정보에 접근\n",
    "soup1.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(url_enter)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup2 = BeautifulSoup(html, 'lxml')\n",
    "print(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title 정보에 접근\n",
    "soup2.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupx = soup2\n",
    "\n",
    "media = soupx.find('meta',attrs={'name':'twitter:creator'}).get('content').strip() # media\n",
    "title = soupx.find('meta',attrs={'property':'og:title'}).get('content').strip() # title\n",
    "url = soupx.find('meta', attrs={'property':'og:url'}).get('content') # URL\n",
    "section = soupx.find('meta', attrs={'name':'twitter:site'}).get('content') # Section\n",
    "gdid = url.split('aid=')[1].split(\"&\")[0]\n",
    "body  = soupx.find('div', attrs={'class':'article_body'}).text.strip() # body\n",
    "\n",
    "try: category = soupx.find('meta', attrs={'property':'me2:category2'}).get('content') # category\n",
    "except: category = ''\n",
    "\n",
    "news_info={'0gdid':gdid, '1title':title, '2url':url, '3section':section, '4category':category, '5body':body}\n",
    "print(news_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_info['5body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복 수행을 위한 함수 작성 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    html = urlopen(url)\n",
    "    time.sleep(1) # 반드시 time.sleep() 시간차 함수를 넣기 바람\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    #print(url)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '네이버'뉴스에 게재된 '연예'뉴스 포맷에만 적용 가능\n",
    "def extract_information_enternews(soupx):\n",
    "    media = soupx.find('meta',attrs={'name':'twitter:creator'}).get('content').strip() # media\n",
    "    title = soupx.find('meta',attrs={'property':'og:title'}).get('content').strip() # title\n",
    "    url = soupx.find('meta', attrs={'property':'og:url'}).get('content') # URL\n",
    "    section = soupx.find('meta', attrs={'name':'twitter:site'}).get('content') # Section\n",
    "    gdid = url.split('aid=')[1].split(\"&\")[0]\n",
    "    body  = soupx.find('div', attrs={'class':'article_body'}).text.strip() # body\n",
    "\n",
    "    try: category = soupx.find('meta', attrs={'property':'me2:category2'}).get('content') # category\n",
    "    except: category = ''\n",
    "            \n",
    "    news_info={'0gdid':gdid, '1title':title, '2url':url, '3section':section, '4category':category, '5body':body}\n",
    "\n",
    "    return news_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수를 사용해 웹페이지 소스에서 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupx_enter = soup2\n",
    "extract_information_enternews(soupx_enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 네이버 뉴스가 아닌 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_not_naver = json_text['items'][2]['link']\n",
    "print(json_text['items'][2])\n",
    "print(url_not_naver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupx_not_naver = get_article(url_not_naver)\n",
    "extract_information_enternews(soupx_not_naver) # 에러 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 뉴스 중, '연예' 뉴스가 아닌 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_social='https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&oid=025&aid=0002944990&sid1=001'\n",
    "print(url_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupx_social = get_article(url_social)\n",
    "extract_information_enternews(soupx_social)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복문을 이용한 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    url_enter = json_text['items'][i]['link']\n",
    "    print(url_enter) # 네이버 연예뉴스가 아닐 경우, 에러 및 중단 발생\n",
    "    soupx_enter = get_article(url_enter)\n",
    "    extract_information_enternews(soupx_enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try-except 구문 활용\n",
    "for i in range(10):\n",
    "    url_enter = json_text['items'][i]['link']\n",
    "    print(url_enter)\n",
    "    try: # 에러가 발생하지 않을 경우, 다음의 명령문 실행\n",
    "        soupx_enter = get_article(url_enter)\n",
    "        print(extract_information_enternews(soupx_enter)) # 추출한 정보 출력\n",
    "    except:\n",
    "        pass  # 에러가 발생할 경우, (중단하지 않고) 다음 i 차례로 넘어감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 예시) 1년치 뉴스를 수집하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_str_day1 = [x for x in ['01.01','01.16','02.01']]\n",
    "url_str_day2 = [x for x in ['01.15','01.31','02.15']]\n",
    "#url_str_day1 = [x for x in ['01.01','01.16','02.01','02.16','03.01','03.16','04.01','04.16','05.01','05.16','06.01','06.16',\n",
    "#                            '07.01','07.16','08.01','08.16','09.01','09.16','10.01','10.16','11.01','11.16','12.01','12.16']]\n",
    "#url_str_day2 = [x for x in ['01.15','01.31','02.15','02.28','03.15','03.31','04.15','04.30','05.15','05.31','06.15','06.30',\n",
    "#                            '07.15','07.31','08.15','08.31','09.15','09.30','10.15','10.31','11.15','11.30','12.15','12.31']]\n",
    "​\n",
    "naver_search_url = []\n",
    "search_result_count_sum = 0\n",
    "for k in range(len(url_str_day1)):\n",
    "    url_search_str0 = 'https://search.naver.com/search.naver?&where=news&query=%22%5B%EB%8B%A8%EB%8F%85%5D%22&sm=tab_pge&sort=2&photo=0&field=1&reporter_article=&pd=3&ds=2018.'\n",
    "    url_search_str_add1 = url_str_day1[k] #'01.01'\n",
    "    url_search_str_add2 = '&de=2018.' + url_str_day2[k] #'12.31'\n",
    "    url_search_str_add3 = '&docid=&nso=so:da,p:from2018' + url_str_day1[k].replace('.','') #'0101'\n",
    "    url_search_str_add4 = 'to2018' + url_str_day2[k].replace('.','') #'1231'\n",
    "    url_search_str_add5 = ',a:t&mynews=101&start='\n",
    "    url_search_str0 = url_search_str0+url_search_str_add1+url_search_str_add2+url_search_str_add3+url_search_str_add4+url_search_str_add5\n",
    "    url_search_firstpage = url_search_str0 + '1&refresh_start=0'\n",
    "    naver_search_url.append(url_search_firstpage)\n",
    "    soup_naver_search_firstpage = get_article(url_search_firstpage)\n",
    "    search_result_pages = soup_naver_search_firstpage.find('div', attrs={'class':'title_desc all_my'}).text.split()[2]\n",
    "    search_result_count = int(search_result_pages.replace('건','').replace(',',''))\n",
    "    search_result_count_sum += search_result_count\n",
    "    search_result_count_pages = int(search_result_count/10)\n",
    "    print(url_search_firstpage, search_result_pages, search_result_count_pages)\n",
    "    #for j in range(0, search_result_count_pages):\n",
    "    for j in range(0, 1):\n",
    "        jx = j*10 + 11\n",
    "        url_search_pages = url_search_str0 + str(jx) + '&refresh_start=0'\n",
    "        naver_search_url.append(url_search_pages)\n",
    "        #print(url_search_pages)\n",
    "print(search_result_count_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
