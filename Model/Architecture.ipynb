{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial import distance\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and fit model to news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_train_fit(data, date, standard=0.93):\n",
    "    theme_vectors_df = pd.read_csv('theme_vectors_xnorm.csv',index_col=0)\n",
    "    theme_vectors_df['vectors'] = theme_vectors_df['vectors'].apply(lambda x: np.array(eval(x)))\n",
    "    \n",
    "    # Set word2vec parameters\n",
    "    tok_path = get_tokenizer()\n",
    "    sp  = SentencepieceTokenizer(tok_path)\n",
    "    v_dimension = 300\n",
    "    v_window = 8\n",
    "    model = Word2Vec.load('word2vec.model')\n",
    "    \n",
    "    # Get news vectors without normalization\n",
    "    def vectorize_without_normal(news):\n",
    "        # Remove letters which are not Hangul\n",
    "        hangul = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]+\")\n",
    "        news_words = hangul.sub(' ', news)\n",
    "        # Tokenization with KoBERT tokenizer\n",
    "        token = sp(news)\n",
    "        final_tokens = token\n",
    "        # Vectorization with word2vec\n",
    "        init_v = np.array([0.0]*v_dimension)\n",
    "        for word in final_tokens:\n",
    "            word_vectors = model.wv\n",
    "            if word in word_vectors.vocab:\n",
    "                v = model.wv[word]\n",
    "                init_v = init_v + v\n",
    "        return init_v\n",
    "    \n",
    "    # Apply vectorization and return cosine simmilarity\n",
    "    def apply_w2v(news):\n",
    "        news_vec = vectorize_without_normal(news)\n",
    "        result = []\n",
    "        # Calculate cosine simmilarity\n",
    "        for theme in theme_vectors_df['vectors']:\n",
    "            cosine = 1 - distance.cosine(theme, news_vec)\n",
    "            result.append(cosine)\n",
    "        df = pd.DataFrame(data=np.zeros([168,2]), columns=['Theme', 'Result'])\n",
    "        df['Theme'] = theme_vectors_df['themes']\n",
    "        df['Result'] = result\n",
    "\n",
    "        df.sort_values('Result', ascending=False, inplace=True)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    theme_list = []\n",
    "    cosine_list = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        result = apply_w2v(data['news'][i])\n",
    "        result = result.sort_values(by='Result', ascending=False).reset_index(drop=True)\n",
    "        theme = result['Theme'][0]\n",
    "        cosine = result['Result'][0]\n",
    "        theme_list.append(theme)\n",
    "        cosine_list.append(cosine)\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(theme_list, cosine_list)), columns = ['Theme', 'Similarity'])\n",
    "    #df = pd.DataFrame(theme_simil, columns = ['Theme', 'Similarity']) \n",
    "    df = pd.concat([data,df],axis=1)\n",
    "    df_filtered = df[df['Similarity']>=standard].reset_index(drop=True)\n",
    "    df_filtered = df_filtered[['Theme','news']]\n",
    "    \n",
    "    # Filter news whose frequency is more than 0 less than 6\n",
    "    df_final = df_filtered.groupby('Theme', as_index=False).count().sort_values(by='news',ascending=False)\n",
    "    df_final = df_final[(df_final['news']>=1) & (df_final['news']<6)].reset_index(drop=True)\n",
    "    print(date)\n",
    "    print(tabulate(df_final, headers='keys', tablefmt='psql'))\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19943 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19943/19943 [03:23<00:00, 97.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-29\n",
      "+----+--------------+--------+\n",
      "|    | Theme        |   news |\n",
      "|----+--------------+--------|\n",
      "|  0 | 방역         |      5 |\n",
      "|  1 | 줄기세포     |      5 |\n",
      "|  2 | 그린뉴딜     |      2 |\n",
      "|  3 | Z플립        |      1 |\n",
      "|  4 | 공공와이파이 |      1 |\n",
      "|  5 | 돼지열병     |      1 |\n",
      "|  6 | 엔터테인먼트 |      1 |\n",
      "|  7 | 키즈         |      1 |\n",
      "|  8 | 폴더블폰     |      1 |\n",
      "|  9 | 해저터널     |      1 |\n",
      "+----+--------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA = pd.read_csv(\"../Data/data_for_use/20200629_news_data.csv\",index_col=0)\n",
    "DATE = '2020-06-29'\n",
    "result = w2v_train_fit(DATA, DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_portfolio(theme, date):\n",
    "    # Load Kospi and Kosdaq list\n",
    "    kospi = pd.read_csv(\"../Data/stock_data/kospi.csv\", encoding='cp949')\n",
    "    kosdaq = pd.read_csv(\"../Data/stock_data/kosdaq.csv\", encoding='cp949')\n",
    "    \n",
    "    # Find target companies which match with theme from theme-comapny list and return compnanies with codes\n",
    "    def target_com(theme, theme_list):\n",
    "        company_list = theme_list[theme_list['Theme']==theme]['Company'].iloc[0]\n",
    "        company_list = str(company_list).replace('[','').replace(']','').replace(\"\\'\",'').replace(' ', '').replace('‘','').replace('’','')\n",
    "        company_list = company_list.split(',')\n",
    "        \n",
    "        # Find comapny code of target companies\n",
    "        target_theme = {}\n",
    "        for target in company_list:\n",
    "            try:\n",
    "                code = kosdaq[kosdaq['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "                target_theme[code] = target\n",
    "            except:\n",
    "                try:\n",
    "                    code = kospi[kospi['기업명']==target].iloc[0]['종목코드']\n",
    "                    code = str(code).zfill(6)\n",
    "                    target_theme[code] = target\n",
    "                except: print('Theme list error occurred!')\n",
    "\n",
    "        return target_theme, company_list\n",
    "\n",
    "    \n",
    "    def target_(target_comapny):\n",
    "        target_theme = {}\n",
    "        for target in target_company:\n",
    "            try:\n",
    "                code = kosdaq[kosdaq['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "            except:\n",
    "                code = kospi[kospi['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "            target_theme[code] = target\n",
    "\n",
    "        return target_theme\n",
    "\n",
    "    \n",
    "    # Convert the input date as 5 business days before the date(start_date) and 1 day before the date(end_date)\n",
    "    def date_convert(date):\n",
    "        end_date = date\n",
    "\n",
    "        def date_by_deducting_business_days(from_date):\n",
    "            business_days_to_deduct = 6\n",
    "            current_date = from_date\n",
    "            while business_days_to_deduct > 0:\n",
    "                current_date += datetime.timedelta(days=-1)\n",
    "                weekday = current_date.weekday()\n",
    "                if weekday >= 5: # sunday = 6\n",
    "                    continue\n",
    "                business_days_to_deduct -= 1\n",
    "            return current_date\n",
    "\n",
    "        convert_end_date = datetime.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        start_date=datetime.date.strftime(date_by_deducting_business_days(convert_end_date),'%Y-%m-%d')\n",
    "\n",
    "        return start_date, end_date\n",
    "\n",
    "    # Date format\n",
    "    def date_format(d):\n",
    "        d = str(d).replace('-', '.')\n",
    "        yyyy = int(d.split('.')[0])\n",
    "        mm = int(d.split('.')[1])\n",
    "        dd = int(d.split('.')[2])\n",
    "\n",
    "        this_date = dt.date(yyyy, mm, dd)\n",
    "        return this_date\n",
    "    def date_format(d=''):\n",
    "        if d != '':\n",
    "            this_date = pd.to_datetime(d).date()\n",
    "        else:\n",
    "            this_date = pd.Timestamp.today().date()\n",
    "        return (this_date)\n",
    "\n",
    "\n",
    "    # Get the information of stock from NAVER\n",
    "    def stock_info(stock_cd):\n",
    "        url_float = 'http://companyinfo.stock.naver.com/v1/company/c1010001.aspx?cmp_cd=' + stock_cd\n",
    "        source = urlopen(url_float).read()\n",
    "        soup = bs(source, 'lxml')\n",
    "\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '').replace('\\n','').replace('\\t','')\n",
    "        tmp = re.split('/', tmp)\n",
    "\n",
    "        outstanding = tmp[0].replace(',','').replace('주','').replace(' ','')\n",
    "        outstanding = int(outstanding)\n",
    "\n",
    "        floating = tmp[1].replace(' ','').replace('%','')\n",
    "        floating = float(floating)\n",
    "\n",
    "        name = soup.find(id='pArea').find('div').find('div').find('tr').find('td').find('span').text\n",
    "\n",
    "        stock_outstanding[stock_cd] = outstanding\n",
    "        stock_floating[stock_cd] = floating\n",
    "        stock_name[stock_cd] = name\n",
    "\n",
    "        return oustanding, floating, name\n",
    "\n",
    "\n",
    "    # Get the historical price of index from NAVER\n",
    "    def historical_index_naver(index_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "        if start_date:\n",
    "            start_date = date_format(start_date)\n",
    "        else:\n",
    "            start_date = datetime.date.today()\n",
    "        if end_date:\n",
    "            end_date = date_format(end_date)+datetime.timedelta(days=-1)\n",
    "        else:\n",
    "            end_date = datetime.date.today()\n",
    "\n",
    "        naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_index).read()\n",
    "        source = bs(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('td', class_='date')\n",
    "        prices = source.find_all('td', class_='number_1')\n",
    "        #historical_prices = dict()\n",
    "\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "            if dates[n].text.split('.')[0].isdigit():\n",
    "\n",
    "                # Handling date\n",
    "                this_date = dates[n].text\n",
    "                this_date = date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:\n",
    "\n",
    "                    # Handling closing price\n",
    "                    this_close = prices[n*4].text\n",
    "                    this_close = this_close.replace(',','')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:\n",
    "                    return historical_prices\n",
    "\n",
    "        # Navigating page\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = int(last_page)\n",
    "\n",
    "        if page_n < last_page:\n",
    "            page_n += 1\n",
    "            historical_index_naver(index_cd, start_date, end_date, page_n, last_page)\n",
    "\n",
    "        return historical_prices\n",
    "\n",
    "    \n",
    "    # Get the historical price of stock from NAVER\n",
    "    def historical_stock_naver(stock_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "        if start_date:\n",
    "            start_date = date_format(start_date)\n",
    "        else:\n",
    "            start_date = datetime.date.today()\n",
    "        if end_date:\n",
    "            end_date = date_format(end_date)+datetime.timedelta(days=-1)\n",
    "        else:\n",
    "            end_date = datetime.date.today()\n",
    "\n",
    "        naver_stock = 'http://finance.naver.com/item/sise_day.nhn?code=' + stock_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_stock).read()\n",
    "        #source = requests.get(naver_stock)\n",
    "        source = bs(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('span', class_='tah p10 gray03')\n",
    "        prices = source.find_all('td', class_='num')\n",
    "        #historical_prices = dict()\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "\n",
    "            if len(dates) > 0:\n",
    "\n",
    "                # Handling date\n",
    "                this_date = dates[n].text\n",
    "                this_date = date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:\n",
    "\n",
    "                    # Handling closing price\n",
    "                    this_close = prices[n*6].text\n",
    "                    this_close = this_close.replace(',','')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:\n",
    "                    return historical_prices\n",
    "\n",
    "\n",
    "        # Navigating page\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = int(last_page)\n",
    "\n",
    "        if page_n < last_page:\n",
    "            page_n += 1\n",
    "            historical_stock_naver(stock_cd, start_date, end_date, page_n, last_page)\n",
    "\n",
    "        return historical_prices\n",
    "\n",
    "\n",
    "    # Set threshold of variance to select stable stock\n",
    "    def apply_threshold(var, threshold=0.05):\n",
    "        if var < -1*threshold or var > threshold: return False\n",
    "        else : return True\n",
    "\n",
    "\n",
    "    def stock_get_info(target_theme):\n",
    "        stock_outstanding = dict()\n",
    "        stock_floating = dict()\n",
    "        stock_name = dict()\n",
    "        for stock_cd in target_theme.keys():\n",
    "            stock_info(stock_cd)\n",
    "\n",
    "        return stock_outstanding, stock_floating, stock_name\n",
    "\n",
    "    \n",
    "    start_date, end_date = date_convert(date)\n",
    "    theme_list=pd.read_csv(\"../Data/data_for_use/theme_company.csv\",index_col=0)\n",
    "    target_theme, company_list = target_com(theme, theme_list)\n",
    "\n",
    "    theme_historical_prices = dict()\n",
    "\n",
    "    for stock_cd in target_theme.keys():\n",
    "        historical_prices = dict()\n",
    "        historical_stock_naver(stock_cd, start_date, end_date)\n",
    "        theme_historical_prices[stock_cd] = historical_prices\n",
    "\n",
    "    theme_historical_price = pd.DataFrame(theme_historical_prices)\n",
    "    theme_historical_price.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    theme_historical_price = theme_historical_price.fillna(method='ffill')\n",
    "    if theme_historical_price.isnull().values.any():\n",
    "        theme_historical_price = theme_historical_price.fillna(method='bfill')\n",
    "\n",
    "    theme_historical_price['kospi200'] = historical_index_naver('KPI200', start_date, end_date).values()\n",
    "    \n",
    "    df = theme_historical_price\n",
    "    # Check change rate\n",
    "    for i in range(len(df.index)-1):\n",
    "        df.iloc[i] = (theme_historical_price.iloc[i+1] - theme_historical_price.iloc[i])/theme_historical_price.iloc[i]\n",
    "    \n",
    "    df.drop(index=[df.index[-1]], inplace=True)\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'{date}의 추천테마는 : {theme}')\n",
    "    print('해당 테마의 추천 종목은 :')\n",
    "    result = {}\n",
    "    # Check whether comapny met fluctuation threshold\n",
    "    for i in range(len(company_list)):\n",
    "        standard = 1\n",
    "        try:\n",
    "            for j in range(len(df.index)-1):\n",
    "                standard *= apply_threshold(df.iloc[:, i][j])\n",
    "\n",
    "            if standard == 1 : \n",
    "                print('{} : {}'.format(df.columns[i], target_theme[df.columns[i]]))\n",
    "                result[df.columns[i]] = target_theme[df.columns[i]]\n",
    "        except:\n",
    "            print('Fluctuation check failed')\n",
    "            pass\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2020-06-29의 추천테마는 : 해저터널\n",
      "해당 테마의 추천 종목은 :\n",
      "028100 : 동아지질\n",
      "060370 : KT서브마린\n"
     ]
    }
   ],
   "source": [
    "portfolio = make_portfolio('해저터널', DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(portfolio, date):\n",
    "    \n",
    "    def date_convert(date):\n",
    "        start_date = date\n",
    "\n",
    "        def date_by_adding_business_days(from_date):\n",
    "            business_days_to_add = 6\n",
    "            current_date = from_date\n",
    "            while business_days_to_add > 0:\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                weekday = current_date.weekday()\n",
    "                if weekday >= 5: # sunday = 6\n",
    "                    continue\n",
    "                business_days_to_add -= 1\n",
    "            return current_date\n",
    "\n",
    "        convert_start_date = datetime.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        end_date=datetime.date.strftime(date_by_adding_business_days(convert_start_date),'%Y-%m-%d')\n",
    "\n",
    "        return start_date, end_date\n",
    "    \n",
    "    \n",
    "    def date_format(d):\n",
    "        d = str(d).replace('-', '.')\n",
    "        yyyy = int(d.split('.')[0])\n",
    "        mm = int(d.split('.')[1])\n",
    "        dd = int(d.split('.')[2])\n",
    "\n",
    "        this_date = dt.date(yyyy, mm, dd)\n",
    "        return this_date\n",
    "    def date_format(d=''):\n",
    "        if d != '':\n",
    "            this_date = pd.to_datetime(d).date()\n",
    "        else:\n",
    "            this_date = pd.Timestamp.today().date()\n",
    "        return (this_date)\n",
    "\n",
    "    def historical_stock_naver(stock_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "    \n",
    "        if start_date:\n",
    "            start_date = date_format(start_date)\n",
    "        else:\n",
    "            start_date = datetime.date.today()\n",
    "        if end_date:\n",
    "            end_date = date_format(end_date)+datetime.timedelta(days=-1)\n",
    "        else:\n",
    "            end_date = datetime.date.today()\n",
    "\n",
    "        naver_stock = 'http://finance.naver.com/item/sise_day.nhn?code=' + stock_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_stock).read()\n",
    "        #source = requests.get(naver_stock)\n",
    "        source = bs(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('span', class_='tah p10 gray03')\n",
    "        prices = source.find_all('td', class_='num')\n",
    "        #historical_prices = dict()\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "\n",
    "            if len(dates) > 0:\n",
    "\n",
    "                # Handle date\n",
    "                this_date = dates[n].text\n",
    "                this_date = date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:\n",
    "\n",
    "                    # Handle closing price\n",
    "                    this_close = prices[n*6].text\n",
    "                    this_close = this_close.replace(',','')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:\n",
    "                    return historical_prices\n",
    "\n",
    "        # Navigate page\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = int(last_page)\n",
    "\n",
    "        if page_n < last_page:\n",
    "            page_n += 1\n",
    "            historical_stock_naver(stock_cd, start_date, end_date, page_n, last_page)\n",
    "\n",
    "        return historical_prices\n",
    "\n",
    "    start_date, end_date = date_convert(date)\n",
    "\n",
    "    theme_historical_prices = dict()\n",
    "    \n",
    "    for stock_cd in portfolio.keys():\n",
    "        historical_prices = dict()\n",
    "        historical_stock_naver(stock_cd, start_date, end_date)\n",
    "        theme_historical_prices[stock_cd] = historical_prices\n",
    "\n",
    "    theme_historical_price = pd.DataFrame(theme_historical_prices)\n",
    "    theme_historical_price.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    theme_historical_price = theme_historical_price.fillna(method='ffill')\n",
    "    if theme_historical_price.isnull().values.any():\n",
    "        theme_historical_price = theme_historical_price.fillna(method='bfill')\n",
    "\n",
    "\n",
    "    #theme_historical_price['kospi200'] = historical_index_naver('KPI200', start_date, end_date).values()\n",
    "    \n",
    "    df = theme_historical_price\n",
    "    # Check increase/decrease rate\n",
    "    for i in range(len(df.index)-1):\n",
    "        df.iloc[i] = (theme_historical_price.iloc[i] - theme_historical_price.iloc[-1])/theme_historical_price.iloc[-1]\n",
    "    df.drop(index=[df.index[-1]], inplace=True)\n",
    "    \n",
    "    # Apply selling policy: sell if 5% decrease or 10% increase\n",
    "    for i in range(len(df.columns)):\n",
    "        for j in range(1, len(df.index)+1):\n",
    "            if (df.iloc[-j, i]>=0.1) or (df.iloc[-j, i]<=-0.05):\n",
    "                df.iloc[0:-j, i] = df.iloc[-j, i]\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "    print(f'Average IRR: {round(df.iloc[0].mean()*100,2)}% for {len(df.index)} days')\n",
    "    return df.iloc[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+\n",
      "|            |    028100 |      060370 |\n",
      "|------------+-----------+-------------|\n",
      "| 2020-07-03 | 0.0611511 |  0.0521845  |\n",
      "| 2020-07-02 | 0.0215827 |  0.0424757  |\n",
      "| 2020-07-01 | 0.0143885 |  0.0291262  |\n",
      "| 2020-06-30 | 0.0107914 | -0.00364078 |\n",
      "+------------+-----------+-------------+\n",
      "Average IRR: 5.67% for 4 days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05666777257805406"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest(portfolio, DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19943 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19943/19943 [03:45<00:00, 88.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-29\n",
      "+----+--------------+--------+\n",
      "|    | Theme        |   news |\n",
      "|----+--------------+--------|\n",
      "|  0 | 방역         |      5 |\n",
      "|  1 | 줄기세포     |      5 |\n",
      "|  2 | 그린뉴딜     |      2 |\n",
      "|  3 | Z플립        |      1 |\n",
      "|  4 | 공공와이파이 |      1 |\n",
      "|  5 | 돼지열병     |      1 |\n",
      "|  6 | 엔터테인먼트 |      1 |\n",
      "|  7 | 키즈         |      1 |\n",
      "|  8 | 폴더블폰     |      1 |\n",
      "|  9 | 해저터널     |      1 |\n",
      "+----+--------------+--------+\n",
      "\n",
      "\n",
      "2020-06-29의 추천테마는 : 폴더블폰\n",
      "해당 테마의 추천 종목은 :\n",
      "055490 : 테이팩스\n",
      "060720 : KH바텍\n",
      "131760 : 파인텍\n",
      "148150 : 세경하이테크\n",
      "179900 : 유티아이\n",
      "+------------+-------------+-------------+------------+------------+-------------+\n",
      "|            |      055490 |      060720 |     131760 |     148150 |      179900 |\n",
      "|------------+-------------+-------------+------------+------------+-------------|\n",
      "| 2020-07-03 |  0.0215517  |  0.00653595 | 0.0501567  |  0.0418327 | -0.0369231  |\n",
      "| 2020-07-02 | -0.00862069 |  0.0130719  | 0.00626959 |  0.0418327 | -0.04       |\n",
      "| 2020-07-01 | -0.00646552 |  0.00653595 | 0          |  0.0318725 | -0.04       |\n",
      "| 2020-06-30 | -0.0431034  | -0.0174292  | 0.015674   | -0.0119522 | -0.00923077 |\n",
      "+------------+-------------+-------------+------------+------------+-------------+\n",
      "Average IRR: 1.66% for 4 days\n"
     ]
    }
   ],
   "source": [
    "DATA = pd.read_csv(\"../Data/data_for_use/20200629_news_data.csv\",index_col=0)\n",
    "DATE = '2020-06-29'\n",
    "w2v = w2v_train_fit(DATA, DATE)\n",
    "portfolio = make_portfolio(w2v.iloc[-2,0], DATE)\n",
    "result = backtest(portfolio, DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
