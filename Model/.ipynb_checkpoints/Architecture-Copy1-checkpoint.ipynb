{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial import distance\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and fit model to news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fit(data, date, standard=0.93):\n",
    "    theme_vectors_df = pd.read_csv('theme_vectors_xnorm.csv',index_col=0)\n",
    "    theme_vectors_df['vectors'] = theme_vectors_df['vectors'].apply(lambda x: eval(x))\n",
    "    theme_vectors_df['vectors'] = theme_vectors_df['vectors'].apply(lambda x: np.array(x))\n",
    "    \n",
    "    # Vectorization parameters\n",
    "    tok_path = get_tokenizer()\n",
    "    sp  = SentencepieceTokenizer(tok_path)\n",
    "    v_dimension = 300\n",
    "    v_window = 8\n",
    "    model = Word2Vec.load('word2vec.model')\n",
    "    \n",
    "    # Get news vectors without normalization\n",
    "    def vectorize_without_normal(news):\n",
    "        # Remove letters which are not Hangul\n",
    "        hangul = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]+\")\n",
    "        news_words = hangul.sub(' ', news)\n",
    "        token = sp(news)\n",
    "        final_tokens = token\n",
    "        init_v = np.array([0.0]*v_dimension)\n",
    "        for word in final_tokens:\n",
    "            word_vectors = model.wv\n",
    "            if word in word_vectors.vocab:\n",
    "                v = model.wv[word]\n",
    "                init_v = init_v + v\n",
    "        return init_v\n",
    "    \n",
    "    # Apply vectorization and return cosine simmilarity\n",
    "    def apply_model(news):\n",
    "        news_vec = vectorize_without_normal(news)\n",
    "        result = []\n",
    "        # Calculate cosine simmilarity\n",
    "        for theme in theme_vectors_df['vectors']:\n",
    "            cosine = 1 - distance.cosine(theme, news_vec)\n",
    "            result.append(cosine)\n",
    "        df = pd.DataFrame(data=np.zeros([168,2]), columns=['Theme', 'Result'])\n",
    "        df['Theme'] = theme_vectors_df['themes']\n",
    "        df['Result'] = result\n",
    "\n",
    "        df.sort_values('Result', ascending=False, inplace=True)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    #theme_lst = []\n",
    "    #simil_lst = []\n",
    "    theme_simil = {}\n",
    "    for i in tqdm(range(len(real_test))):\n",
    "        result = apply_model(data['news'][i])\n",
    "        result = result.sort_values(by='Result', ascending=False).reset_index(drop=True)\n",
    "        theme = result['Theme'][0]\n",
    "        cosine = result['Result'][0]\n",
    "        theme_simil[theme] = cosine\n",
    "        #lst.append(theme)\n",
    "        #lst2.append(cosine)\n",
    "        \n",
    "    #df = pd.DataFrame(list(zip(theme_lst, simil_lst)), columns = ['Theme', 'Similarity'])\n",
    "    df = pd.DataFrame(theme_simil, columns = ['Theme', 'Similarity']) \n",
    "    df = pd.concat([data,df],axis=1)\n",
    "    df_filtered = df[df['Similarity']>=standard].reset_index(drop=True)\n",
    "    df_filtered = df_filtered[['Theme','news']]\n",
    "    \n",
    "    # Filter news whose frequency is more than 0 less than 6\n",
    "    df_final = df_filtered.groupby('Theme', as_index=False).count().sort_values(by='news',ascending=False)\n",
    "    df_final = df_final[(df_final['news']>=1) & (df_final['news']<6)].reset_index(drop=True)\n",
    "    print(date, df_final)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/data_for_use/20200629_news_data.csv\",index_col=0)\n",
    "date = '2020-06-29'\n",
    "train_fit(data, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_portfolio(theme, test_date):\n",
    "    # Load Kospi and Kosdaq list\n",
    "    kospi = pd.read_csv(\"../Data/stock_data/kospi.csv\", encoding='cp949')\n",
    "    kosdaq = pd.read_csv(\"../Data/stock_data/kosdaq.csv\", encoding='cp949')\n",
    "    \n",
    "    # Find target companies which match with theme from theme-comapny list and return compnanies with codes\n",
    "    def target_com(theme, theme_list):\n",
    "        company_list = theme_list[theme_list['Theme']==theme]['Company'].iloc[0]\n",
    "        company_list = str(company_list).replace('[','').replace(']','').replace(\"\\'\",'').replace(' ', '').replace('‘','').replace('’','')\n",
    "        comapny_list = company_list.split(',')\n",
    "        \n",
    "        # Find comapny code of target companies\n",
    "        target_theme = {}\n",
    "        for target in company_list:\n",
    "            try:\n",
    "                code = kosdaq[kosdaq['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "                target_theme[code] = target\n",
    "            except:\n",
    "                try:\n",
    "                    code = kospi[kospi['기업명']==target].iloc[0]['종목코드']\n",
    "                    code = str(code).zfill(6)\n",
    "                    target_theme[code] = target\n",
    "                except: print('Theme list error occurred!')\n",
    "\n",
    "        return target_theme, company_list\n",
    "\n",
    "    \n",
    "    def target_(target_comapny):\n",
    "        target_theme = {}\n",
    "        for target in target_company:\n",
    "            try:\n",
    "                code = kosdaq[kosdaq['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "            except:\n",
    "                code = kospi[kospi['기업명']==target].iloc[0]['종목코드']\n",
    "                code = str(code).zfill(6)\n",
    "            target_theme[code] = target\n",
    "\n",
    "        return target_theme\n",
    "\n",
    "    \n",
    "    # Convert the input date as 5 business days before the date(start_date) and 1 day before the date(end_date)\n",
    "    def date_convert(date):\n",
    "        end_date = date\n",
    "\n",
    "        def date_by_deducting_business_days(from_date):\n",
    "            business_days_to_deduct = 6\n",
    "            current_date = from_date\n",
    "            while business_days_to_deduct > 0:\n",
    "                current_date += datetime.timedelta(days=-1)\n",
    "                weekday = current_date.weekday()\n",
    "                if weekday >= 5: # sunday = 6\n",
    "                    continue\n",
    "                business_days_to_deduct -= 1\n",
    "            return current_date\n",
    "\n",
    "        convert_end_date = datetime.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        start_date=datetime.date.strftime(date_by_deducting_business_days(convert_end_date),'%Y-%m-%d')\n",
    "\n",
    "        return start_date, end_date\n",
    "\n",
    "    # Date format\n",
    "    def date_format(d):\n",
    "        d = str(d).replace('-', '.')\n",
    "        yyyy = int(d.split('.')[0])\n",
    "        mm = int(d.split('.')[1])\n",
    "        dd = int(d.split('.')[2])\n",
    "\n",
    "        this_date = dt.date(yyyy, mm, dd)\n",
    "        return this_date\n",
    "    def date_format(d=''):\n",
    "        if d != '':\n",
    "            this_date = pd.to_datetime(d).date()\n",
    "        else:\n",
    "            this_date = pd.Timestamp.today().date()\n",
    "        return (this_date)\n",
    "\n",
    "\n",
    "    # Get the information of stock from NAVER\n",
    "    def stock_info(stock_cd):\n",
    "        url_float = 'http://companyinfo.stock.naver.com/v1/company/c1010001.aspx?cmp_cd=' + stock_cd\n",
    "        source = urlopen(url_float).read()\n",
    "        soup = bs(source, 'lxml')\n",
    "\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '').replace('\\n','').replace('\\t','')\n",
    "        tmp = re.split('/', tmp)\n",
    "\n",
    "        outstanding = tmp[0].replace(',','').replace('주','').replace(' ','')\n",
    "        outstanding = int(outstanding)\n",
    "\n",
    "        floating = tmp[1].replace(' ','').replace('%','')\n",
    "        floating = float(floating)\n",
    "\n",
    "        name = soup.find(id='pArea').find('div').find('div').find('tr').find('td').find('span').text\n",
    "\n",
    "        stock_outstanding[stock_cd] = outstanding\n",
    "        stock_floating[stock_cd] = floating\n",
    "        stock_name[stock_cd] = name\n",
    "\n",
    "        return oustanding, floating, name\n",
    "\n",
    "\n",
    "    # Get the historical price of index from NAVER\n",
    "    def historical_index_naver(index_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "        if start_date:\n",
    "            start_date = date_format(start_date)\n",
    "        else:\n",
    "            start_date = datetime.date.today()\n",
    "        if end_date:\n",
    "            end_date = date_format(end_date)+datetime.timedelta(days=-1)\n",
    "        else:\n",
    "            end_date = datetime.date.today()\n",
    "\n",
    "        naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_index).read()\n",
    "        source = bs(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('td', class_='date')\n",
    "        prices = source.find_all('td', class_='number_1')\n",
    "        #historical_prices = dict()\n",
    "\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "            if dates[n].text.split('.')[0].isdigit():\n",
    "\n",
    "                # Handling date\n",
    "                this_date = dates[n].text\n",
    "                this_date = date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:\n",
    "\n",
    "                    # Handling closing price\n",
    "                    this_close = prices[n*4].text\n",
    "                    this_close = this_close.replace(',','')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:\n",
    "                    return historical_prices\n",
    "\n",
    "        # Navigating page\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = int(last_page)\n",
    "\n",
    "        if page_n < last_page:\n",
    "            page_n += 1\n",
    "            historical_index_naver(index_cd, start_date, end_date, page_n, last_page)\n",
    "\n",
    "        return historical_prices\n",
    "\n",
    "    \n",
    "    # Get the historical price of stock from NAVER\n",
    "    def historical_stock_naver(stock_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "        if start_date:\n",
    "            start_date = date_format(start_date)\n",
    "        else:\n",
    "            start_date = datetime.date.today()\n",
    "        if end_date:\n",
    "            end_date = date_format(end_date)+datetime.timedelta(days=-1)\n",
    "        else:\n",
    "            end_date = datetime.date.today()\n",
    "\n",
    "        naver_stock = 'http://finance.naver.com/item/sise_day.nhn?code=' + stock_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_stock).read()\n",
    "        #source = requests.get(naver_stock)\n",
    "        source = bs(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('span', class_='tah p10 gray03')\n",
    "        prices = source.find_all('td', class_='num')\n",
    "        #historical_prices = dict()\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "\n",
    "            if len(dates) > 0:\n",
    "\n",
    "                # Handling date\n",
    "                this_date = dates[n].text\n",
    "                this_date = date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:\n",
    "\n",
    "                    # Handling closing price\n",
    "                    this_close = prices[n*6].text\n",
    "                    this_close = this_close.replace(',','')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:\n",
    "                    return historical_prices\n",
    "\n",
    "\n",
    "        # Navigating page\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = int(last_page)\n",
    "\n",
    "        if page_n < last_page:\n",
    "            page_n += 1\n",
    "            historical_stock_naver(stock_cd, start_date, end_date, page_n, last_page)\n",
    "\n",
    "        return historical_prices\n",
    "\n",
    "\n",
    "    # Set threshold of variance to select stable stock\n",
    "    def threshold(var):\n",
    "        thres = 0.05\n",
    "        if var < -1*thres or var > thres: return False\n",
    "        else : return True\n",
    "\n",
    "\n",
    "    def stock_get_info(target_theme):\n",
    "        stock_outstanding = dict()\n",
    "        stock_floating = dict()\n",
    "        stock_name = dict()\n",
    "        for stock_cd in target_theme.keys():\n",
    "            stock_info(stock_cd)\n",
    "\n",
    "        return stock_outstanding, stock_floating, stock_name\n",
    "\n",
    "    \n",
    "    start_date, end_date = date_convert(test_date)\n",
    "    theme_list=pd.read_csv(\"../Data/data_for_use/theme_company.csv\",index_col=0)\n",
    "    target_theme, company_list = target_com(theme, theme_list)\n",
    "\n",
    "    theme_historical_prices = dict()\n",
    "\n",
    "    for stock_cd in target_theme.keys():\n",
    "        historical_prices = dict()\n",
    "        historical_stock_naver(stock_cd, start_date, end_date)\n",
    "        theme_historical_prices[stock_cd] = historical_prices\n",
    "\n",
    "    theme_historical_price = pd.DataFrame(theme_historical_prices)\n",
    "    theme_historical_price.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    theme_historical_price = theme_historical_price.fillna(method='ffill')\n",
    "    if theme_historical_price.isnull().values.any():\n",
    "        theme_historical_price = theme_historical_price.fillna(method='bfill')\n",
    "\n",
    "    theme_historical_price['kospi200'] = historical_index_naver('KPI200', start_date, end_date).values()\n",
    "    \n",
    "    df = theme_historical_price\n",
    "    for i in range(len(df.index)-1):\n",
    "        df.iloc[i] = (theme_historical_price.iloc[i] - theme_historical_price.iloc[i+1])/theme_historical_price.iloc[i]\n",
    "    \n",
    "    df.drop(index=[df.index[-1]], inplace=True)\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('오늘의 추천테마는 : {}'.format(theme))\n",
    "    print('해당 테마의 추천 종목은 :')\n",
    "    for i in range(len(company_list)):\n",
    "        standard = 1\n",
    "        try:\n",
    "            for j in range(len(df.index)-1):\n",
    "                standard *= threshold(df.iloc[:, i][j])\n",
    "\n",
    "            if standard == 1 : \n",
    "                print('{} : {}'.format(df.columns[i], target_theme[df.columns[i]]))\n",
    "        except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "오늘의 추천테마는 : 해저터널\n",
      "해당 테마의 추천 종목은 :\n",
      "028100 : 동아지질\n",
      "060370 : KT서브마린\n"
     ]
    }
   ],
   "source": [
    "make_portfolio('해저터널', '2020-06-29')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
